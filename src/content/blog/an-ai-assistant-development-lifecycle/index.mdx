---
title: An AI Assistant Development Lifecycle
pubDate: 2025-02-04T15:10:05.369Z
heroImage: [./assets/featured-image.svg, Placeholder featured image]
description: How to implement and administer AI Assistants locally
repo: https://github.com/andersr/structured-response-demo
isDraft: true
---

In my previous post, I talked about enabling fully typed output from an AI Assistant. One of the requirements for achieving this is that the schema used for the assistant output needs to be co-located with other app code, so we can use the same schema both for  defining types in the app and for defining the assistant output.  This means we also need to create and update assistants in the same environment as the app, i.e. locally (as opposed to in the [OpenAI dashboard](https://platform.openai.com/).)  In turn, because we are creating assistants locally (via the OpenAI API), we will also need to store the ids that are returned, so we then can update assistants as needed.

To achieve this, we can set up a data store for the assistant ids, as well as a set of commands for managing the assistant lifecycle (create, update, etc.).  In this post, I'll walk through my implementation of this, using the [Trivia Game App](https://github.com/andersr/structured-response-demo) from previous posts, as an example of assistant lifecycle management.

## An AI Assistant Development Lifecycle 

The specifics of any lifecycle will vary depending on needs and preferences. This is the lifecycle I've neen using when implementing a feature that leverages structured output from an AI Assistant.

INSERT A LIFECYCLE VISUAL 

- Create an internal app id
- Create assistant instances for each app environment (dev, prod, staging, test, etc.).
- Iterate on the dev assistant until the output is as desired.
- "Deploy" assistant config 
- Repeat iterate/deploy steps as needed.

## Create an internal app id

As I discussed in a previous post, because assistants that emit structured output can only leverage a single schema, they should therefore also be instructed to have a single output goal. In the case of our example app, one main feature of the app is to create trivia questions for a new game. Therefore, a good id for the assistant that generates these questions might be `createTriviaQuestions`. The id should be descriptive of the purpose and unique among assistants for that app. The id will then be used as the base name for the environment-specific assistants, and will also be used to store and retrieve the corresponding OpenAI ids.

Here is an example of defining the app ids in a read-only array and then using that as a basis for an Assistant Id type. 

```ts
export const ASSISTANT_IDS = ["createTriviaGame"] as const;

export type AssistantId = (typeof ASSISTANT_IDS)[number];
```

Next, we'd want to create assistants using the OpenAI API for each of the environments, but first let's talk about how we are going to stores the ids that are returned.

## Storing Assistant Ids 

As mentioned above, since we are managing assistants locally, we therefore also need a way to store the ids, so we can later update assistants and use them to generate output.

You can choose whatever data store you prefer to persist the ids. I suggest leveraging whatever you already are using, eg Postgres, S3, etc. In my case, I went with a Redis store, mostly because I already had that set up in the app.

In terms of data structure, use the internally defined assistant id we created above as the primary key, and associate this with a collection of environment/assistant id key/value pairs, e.g.:

```ts
createTriviaGame: {
    development: "<OpenAI Asst Id for dev env>",
    production: "<OpenAI Asst Id for prod env>",
    // other envs, as needed
}
```

With this in place, we now have the basis for adding lifecycle commands. 

## When and where should assistant lifecycle commands be run?

When I started working on this, I quickly realized that creating and updating of assistants needs to happen outside the running app itself. In other words, you wouldn't want to create a new assistant, say, when the app boots, as that would result in innumerable duplicate assistants.  At the same time, since the app creation handler needs access to the same schema needed for app types, we will want lifecycle management to be co-located with the app code.

The simplest solution, I've found, is to define an assistant admin function that accepts an "ACTION" input, which determines which lifecycle handler to call, and then to simply add command scripts that can be run as needed. In my case, I added these as `npm` scripts:

```sh title="package.json"
...
  "scripts": {
    ...
    "asst:admin": "dotenv -- tsx app/admin/assistants.server.ts",
    "asst:create": "ACTION=create npm run asst:admin",
    "asst:update": "ACTION=update npm run asst:admin",
    "asst:deploy": "NODE_ENV=production ACTION=update npm run asst:admin"
  },
...
```
In this example, I've added support for creating, updating, and "deploying" (really just propagating changes to all assistants) commands.  I used [`dotenv-cli`](https://www.npmjs.com/package/dotenv-cli) for enabling access to env variables (for access to our data store), and [tsx](https://www.npmjs.com/package/tsx) so scripts could be written in Typescript.

## Create assistant instances for each app environment

After having defined our internal assistant id (see above), our first step in the lifecycle is to create OpenAI assistants for each environment (dev, prod, staging, etc.) and then associate their ids in our data store. At this point, we can just add the minimally required config to the assistant, as all we really care about is getting and storing the ids, so that we'll be able to later update their configuration during the development cycle.

(INSERT CODE SNIPPET?)

## Iterate on the dev assistant until the output is as desired 

During development, while iterating on the assistant config, update the dev version of your assistant.

## "Deploy" assistant config

Once everything looks good, we now want to take our assistant for a spin in a deployed environment.
With the "deploy" command, we propagate the dev assistant configuration to other environments.


## Repeat iterate/deploy steps as needed.
With these three commands, I can support a complete development lifecycle.

One of the many benefits of this model is that achieving end to end type safety is much easier. Another is that you now have all your assistant config co-located with your code, which IMO makes development much easier.


