---
title: An AI Assistant Development Lifecycle
pubDate: 2025-02-04T15:10:05.369Z
heroImage: [./assets/featured-image.jpg, A playful visualization of a development lifecycle]
heroImageCredit: [ChatGPT, ""]
description: How to implement and administer AI Assistants locally
repo: https://github.com/andersr/structured-response-demo
isDraft: true
---

In my previous post, I talked about enabling fully typed output from an AI Assistant. One of the requirements for achieving this is that the schema used for the assistant output needs to be co-located with other app code, so we can use the same schema both for  defining types in the app and for defining the assistant output.

This means we also need to create and update assistants in the same environment as the app, i.e. locally (as opposed to in the [OpenAI dashboard](https://platform.openai.com/).)  In turn, because we are creating assistants locally (via the OpenAI API), we will also need to store the ids that are returned, so we then can update assistants as needed.

To achieve this, we can set up a data store for the assistant ids, as well as a set of commands for managing the assistant lifecycle (create, update, etc.).  In this post, I'll walk through my implementation of assistant lifecycle management, using the [Trivia Game App](https://github.com/andersr/structured-response-demo) from previous posts as an example.

## The Development Lifecycle 

The specifics of any lifecycle will vary depending on needs and preferences. This is the lifecycle I've neen using when implementing a feature that leverages structured output from an AI Assistant.

1. Create an internal app name
2. Create assistant instances for each app environment
3. Iterate on updating the dev assistant and propagating updates to other environments as needed.

## Create an internal app name

As I discussed in a previous post, because assistants that emit structured output can only leverage a single schema, they should therefore also be instructed to have a single output goal. In the case of our example app, one main feature of the app is to create trivia questions for a new game. Therefore, a good name for that assistant might be `createTriviaQuestions`.

The name should be descriptive of the purpose and unique among assistants for that app. It will then be used as the internal id and base name for the environment-specific assistants, and will also be used to store and retrieve the corresponding OpenAI assistant ids.

Here is an example of defining the app ids in a read-only array and then using that as a basis for an Assistant Id type. 

```ts
export const ASSISTANT_NAMES = ["createTriviaGame"] as const;

export type AssistantName = (typeof ASSISTANT_NAMES)[number];
```

Even though these names effectively will be used as ids, I am using "name" to disambiguate from the OpenAI assistant Ids.

Next, we'd want to create assistants using the OpenAI API for each of the environments, but first let's talk about how we are going to stores the ids that are returned.

## Storing Assistant Ids 

As mentioned above, since we are managing assistants locally, we therefore also need a way to store the ids, so we can later update assistants and use them to generate output.

Choose whatever type of store you prefer to persist the ids. If you already have something in place for your app, eg Postgres or S3, I suggest using that. In my case, I went with a Redis store, mostly because I already had that set up in the app.

In terms of data structure, you'll want to associate the ids for all your environments for a given assistant with your internally defined assistant name. This is the structure I went with:

```ts
createTriviaGame: {
    development: "<OpenAI Asst Id for dev>",
    production: "<OpenAI Asst Id for prod>",
    // other envs, as needed
}
```

With this in place, we now have the basis for adding lifecycle commands. 

## When and where should assistant lifecycle commands be run?

When I started working on this, I quickly realized that creating and updating of assistants needs to happen outside the running app itself. In other words, you wouldn't want to create a new assistant, say, when the app boots, as that would result in innumerable duplicate assistants.  At the same time, since the app creation handler needs access to the same schema needed for app types, we will want lifecycle management to be co-located with the app code.

The simplest solution, I've found, is to define an assistant admin function that accepts an "ACTION" input, which determines which lifecycle handler to call, and then to simply add command scripts that can be run as needed. In my case, I added these as `npm` scripts:

```sh title="package.json"
...
  "scripts": {
    ...
    "asst:admin": "dotenv -- tsx app/admin/assistants.server.ts",
    "asst:create": "ACTION=create npm run asst:admin",
    "asst:update": "ACTION=update npm run asst:admin",
    "asst:deploy": "NODE_ENV=production ACTION=update npm run asst:admin"
  },
...
```
In this example, I've created a base `admin` script and then call that script and pass in the necessary variables.  I added scripts for creating, updating, and "deploying" (really just propagating changes to all assistants).  I used [`dotenv-cli`](https://www.npmjs.com/package/dotenv-cli) for enabling access to env variables (for access to our data store), and [tsx](https://www.npmjs.com/package/tsx) so scripts could be written in Typescript.

## Running the lifecycle commands 

The lifecycle commands should at this point be relatively self-explanatory.  We run the `asst:create` command whenever a new assistant is added, run the `asst:update` as needed while iterating on the assistant config, and `asst:deploy` when we are ready to copy our local configs to the other environment assistants, in order to run and test in a deployed environment.

Take a look at the repo code for a more detailed understanding of the lifecycle implementation.

While I previously always started my assistant development process by going to the dashboard, this lifecycle implementation has allowed me to instead do all my work directly in the codebase.  I hope you find it useful.

